https://zhuanlan.zhihu.com/p/376486858

主流的CNN模型都是采用金字塔结构或者层级结构，即将模型分成不同stage，每个stage都会降低特征图大小（feature map  size）但同时提升特征数量（channels）。但是ViT等模型却没有stage的概念，所有的layers都是采用同样的参数配置，其输入也是同样维度的特征。对于图像来说，这样的设计从计算量来看并没太友好，所以一些最近的工作如[PVT](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2102.12122)就采用了金字塔结构来构造vision transformer，并在速度和效果取得了较好的balance。Swin Transformer设计的金字塔结构和PVT基本一致，网络包括一个patch partition和4个stage：



1. patch partition：将图像拆分成  $\frac{H}{4} \times \frac{W}{4}$  个patch，每个patch大小为 $4 \times 4$（ViT一般为 $16 \times 16$或 $32 \times 32$；
2. stage1开始是一个patch embedding操作：通过linear embedding层得到![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7BHW%7D%7B16%7D%5Ctimes+C)的patch embeddings，通过卷积来实现； 
3. 剩余的3个stage开始都先有一个patch merging：将每个![[公式]](https://www.zhihu.com/equation?tex=2%5Ctimes+2)相邻patchs的特征合并，此时特征维度大小 4C，然后通过一个linear层映射到 2C 的特征空间。这个过程patchs的数量降低4x，而特征增长2x，和CNN的stride=2的downsample类似。其实这个patch merging也等价于对![[公式]](https://www.zhihu.com/equation?tex=2%5Ctimes+2)区域做patch embedding。具体实现如下：

```python
class PatchMerging(nn.Module):
    r""" Patch Merging Layer.
    Args:
        input_resolution (tuple[int]): Resolution of input feature.
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(4 * dim)

    def forward(self, x):
        """
        x: B, H*W, C
        """
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"
        assert H % 2 == 0 and W % 2 == 0, f"x size ({H}*{W}) are not even."

        x = x.view(B, H, W, C)

        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.norm(x)
        x = self.reduction(x)

        return x

```

​	4. 然后每个stage包含相同配置的transformer blocks，stage1到stage4的特征图分辨率分别是原图的![[公式]](https://www.zhihu.com/equation?tex=1%2F4)，![[公式]](https://www.zhihu.com/equation?tex=1%2F8)，![[公式]](https://www.zhihu.com/equation?tex=1%2F16)，![[公式]](https://www.zhihu.com/equation?tex=1%2F32)，这样Swin Transformer就非常容易应用在基于FPN的dense prediction任务中； （5)最后对所有的patch  embeddings求平均，即CNN中常用的global average pooling，然后送入一个linear  classifier进行分类。这和ViT采用class token来做分类不一样。

 

**Window Attention**

ViT中的self-attention是对所有的tokens进行attention，这样虽然可以建立tokens间的全局联系，但是计算量和tokens数量的平方成正比。Swin Transformer提出采用window attention来降低计算量，首先将特征图分成互不重叠的window，每个window包含相邻的![[公式]](https://www.zhihu.com/equation?tex=M%5Ctimes+M)个patchs，每个window内部单独做self-attention，这可看成是一种local attention方法。对于一个包含![[公式]](https://www.zhihu.com/equation?tex=h%5Ctimes+w)个patchs的图像来说，基于window的attention方法（W-MSA）和原始的MSA计算复杂度对比如下：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%E2%84%A6%28MSA%29+%3D+4hwC%5E2+%2B+2%28hw%29+%5E2C+%5Cnotag+%5C%5C+%E2%84%A6%28W-MSA%29+%3D+4hwC%5E2+%2B+2M%5E2hwC+%5Cnotag+%5Cend%7Balign%7D+%5C%5C)

可以看到MSA的计算量与图像大小平方-![[公式]](https://www.zhihu.com/equation?tex=%28hw%29%5E2)成正比，而由于window大小![[公式]](https://www.zhihu.com/equation?tex=M)是一个固定值（论文中默认为7，相比图像大小较小），所以W-MSA的计算量和图像大小平方-![[公式]](https://www.zhihu.com/equation?tex=%28hw%29)成正比，这对于高分辨率的图像，计算量大大降低。另外，各个window的参数是共享的，这个和卷积的kernel类似，所以W-MSA就和convolution一样具有locality和parameter sharing 两大特性。对于W-MSA，首先要实现window  partition和reverse，具体实现如下（由于参数共享，可以将num_windows并入Batch维度）：



```python
def window_partition(x, window_size):
    """
    Args:
        x: (B, H, W, C)
        window_size (int): window size
    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    """
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows


def window_reverse(windows, window_size, H, W):
    """
    Args:
        windows: (num_windows*B, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image
    Returns:
        x: (B, H, W, C)
    """
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x

```

由于self-attention是permutation-invariant的，所以需要引入position  embedding来增加位置信息。Swin  Transformer采用的是一种相对位置编码，具体的是在计算query和key的相似度时加一个relative position bias：

![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BAttention%7D%28Q%2C+K%2C+V+%29+%3D+%5Ctext%7BSoftMax%7D%28QK%5ET%5Csqrt+d+%2B+B%29V+%5C%5C)

这里的![[公式]](https://www.zhihu.com/equation?tex=Q%2CK%2CV%5Cin+%5Cmathbb%7BR%7D%5E%7BM%5E2%5Ctimes+d%7D)为query，key和value，![[公式]](https://www.zhihu.com/equation?tex=M%5E2)就是一个window里的patchs总数，而![[公式]](https://www.zhihu.com/equation?tex=B%5Cin+%5Cmathbb%7BR%7D%5E%7BM%5E2%5Ctimes+M%5E2%7D)是relative position bias，用来表征patchs间的相对位置，attention mask与之相加后就能引入位置信息了。但是实际上我们并需要定义那么大的参数，由于一个window里的tokens在![[公式]](https://www.zhihu.com/equation?tex=h)和![[公式]](https://www.zhihu.com/equation?tex=w)每个维度上的相对位置都在![[公式]](https://www.zhihu.com/equation?tex=%5B%E2%88%92M+%2B+1%2C+M+%E2%88%921%5D)范围内，共有![[公式]](https://www.zhihu.com/equation?tex=2M-1)个取值，如果我们采用2D relative position来编码的话，相对位置共有![[公式]](https://www.zhihu.com/equation?tex=%282M-1%29%5Ctimes+%282M-1%29)  ，那么只需要定义![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7BB%7D%5Cin+%5Cmathbb%7BR%7D%5E%7B%282M-1%29%5Ctimes+%282M-1%29%7D)的relative position bias就可以了，这样就降低了参数量（论文中每个W-MSA都独有自己的relative position bias），实际的![[公式]](https://www.zhihu.com/equation?tex=B)通过索引从![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7BB%7D)中得到。从论文中的实验来看（如下所示），直接采用relative position效果是最好的，如果加上ViT类似的abs. pos.，虽然分类效果一致，但是分割效果下降。





### **Shifted Window**



window attention毕竟是一种local attention，如果每个stage采用相同的window  attention，那么信息交换只存在每个window内部。用CNN的话语说，那么感受野是没有发生变化的，此时只有当进入下一个stage后，感受野才增大2倍。论文中提出的解决方案是采用shifted window来建立windows间的信息交互。常规的window切分是从特征图的左上角开始均匀地切分，比如下图中的![[公式]](https://www.zhihu.com/equation?tex=8%5Ctimes+8)大小的特征图切分成4个windows，这里![[公式]](https://www.zhihu.com/equation?tex=M%3D4)。那么shifted window是从![[公式]](https://www.zhihu.com/equation?tex=h)和![[公式]](https://www.zhihu.com/equation?tex=w)两个维度各shift ![[公式]](https://www.zhihu.com/equation?tex=%5Clfloor%7B%5Cfrac%7BM%7D%7B2%7D%7D%5Crfloor)个patchs，此时切分的windows如下图所示，注意此时边缘部分产生的windows就不再是![[公式]](https://www.zhihu.com/equation?tex=M%5Ctimes+M)大小的。通过shifted window进行的window attention记为SW-MSA，当交替地进行W-MSA和SW-MSA，模型的表达能力就会增强，因为windows间也有了信息传递。



![image-20210805172024337](E:\typora 截图\image-20210805172024337.png)

但是shifted window会带来一个问题：边缘处的windows的大小发生了改变，而且这还会导致windows的数量从![[公式]](https://www.zhihu.com/equation?tex=%5Clceil%7B%5Cfrac%7Bh%7D%7BM%7D%7D%5Crceil%5Ctimes+%5Clceil%7B%5Cfrac%7Bw%7D%7BM%7D%7D%5Crceil)增加至![[公式]](https://www.zhihu.com/equation?tex=%28%5Clceil%7B%5Cfrac%7Bh%7D%7BM%7D%7D%5Crceil%2B1%29%5Ctimes+%28%5Clceil%7B%5Cfrac%7Bw%7D%7BM%7D%7D%5Crceil%2B1%29)。在实现SW-MSA时，一种最直接的处理方式是对边缘进行padding，使得所有的windows的大小为![[公式]](https://www.zhihu.com/equation?tex=M%5Ctimes+M)，这样就可以像W-MSA一样将windows组成batch进行计算，但是这会增加计算量，特别是当windows总数较少时。比如上图中例子，W-MSA的windows数量为![[公式]](https://www.zhihu.com/equation?tex=2%5Ctimes+2)，而SW-MSA的windows数量为![[公式]](https://www.zhihu.com/equation?tex=3%5Ctimes+3)，这样计算量将变为原来的2.25倍。论文中提出了一种cyclic-shift的策略来高效地实现SW-MSA，如下图所示，直观来看是讲左上的6个windows移动到右下位置，这样处理后8个较小的windows就可以组成3个大小为![[公式]](https://www.zhihu.com/equation?tex=M%5Ctimes+M)的windows，那么总的windows数量就没有变化，计算量一样，完成attention计算后reverse就好。这种cyclic-shift可以通过`torch.roll`来实现。虽然小的windows可以组成常规windows，但是在attention计算时要通过mask来保证原来的效果。生成attention mask的实现也是比较简单：所有的windows可以分成9组，其中边缘部分共8个windows，而中间的windows都是大小为![[公式]](https://www.zhihu.com/equation?tex=M%5Ctimes+M)的正常windows，可看成1组；9组可以设定不同的id，在组成新的windows后id不同的tokens就通过mask不进行attention计算。

![img](https://pic2.zhimg.com/80/v2-e95e67d557384ae989cf6fe592fa94d1_1440w.jpg)



### **Swin Transformer**

Swin Transformer的核心就是在每个stage交替采用W-MSA和SW-MSA，其中window size ![[公式]](https://www.zhihu.com/equation?tex=M%3D7)，论文中共设计了4种不同的模型：**Swin-T, Swin-S, Swin-B，Swin-L**，具体的参数如下所示：

- Swin-T: C = 96, layer numbers = {2, 2, 6, 2} 
- Swin-S: C = 96, layer numbers ={2, 2, 18, 2} 
- Swin-B: C = 128, layer numbers ={2, 2, 18, 2} 
- Swin-L: C = 192, layer numbers ={2, 2, 18, 2}

不同模型主要是特征维度不同和每个stage的层数不同，其中Swin-T和Swin-S的模型复杂度类比ResNet-50 (DeiT-S) 和ResNet-101。模型中的MSA每个head的特征维度![[公式]](https://www.zhihu.com/equation?tex=d+%3D+32)（变化的只是heads数量），FFN中的expansion系数![[公式]](https://www.zhihu.com/equation?tex=%CE%B1+%3D+4)。

SwinT模型在ImageNet上与其它模型的对比如下所示，可以看到SwinT效果要优于DeiT，相比CNN网络RegNet和EfficientNet也有较好的speed-accuracy trade-off。
















# Swin Transformer: Hierarchical Vision Transformer using Shifted Windows





https://zhuanlan.zhihu.com/p/376486858

主流的CNN模型都是采用金字塔结构或者层级结构，即将模型分成不同stage，每个stage都会降低特征图大小（feature map  size）但同时提升特征数量（channels）。但是ViT等模型却没有stage的概念，所有的layers都是采用同样的参数配置，其输入也是同样维度的特征。对于图像来说，这样的设计从计算量来看并没太友好，所以一些最近的工作如[PVT](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2102.12122)就采用了金字塔结构来构造vision transformer，并在速度和效果取得了较好的balance。Swin Transformer设计的金字塔结构和PVT基本一致，网络包括一个patch partition和4个stage：



1. patch partition：将图像拆分成  $\frac{H}{4} \times \frac{W}{4}$  个patch，每个patch大小为 $4 \times 4$（ViT一般为 $16 \times 16$或 $32 \times 32$；
2. stage1开始是一个patch embedding操作：通过linear embedding层得到![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7BHW%7D%7B16%7D%5Ctimes+C)的patch embeddings，通过卷积来实现； 
3. 剩余的3个stage开始都先有一个patch merging：将每个![[公式]](https://www.zhihu.com/equation?tex=2%5Ctimes+2)相邻patchs的特征合并，此时特征维度大小 4C，然后通过一个linear层映射到 2C 的特征空间。这个过程patchs的数量降低4x，而特征增长2x，和CNN的stride=2的downsample类似。其实这个patch merging也等价于对![[公式]](https://www.zhihu.com/equation?tex=2%5Ctimes+2)区域做patch embedding。具体实现如下：

```python
class PatchMerging(nn.Module):
    r""" Patch Merging Layer.
    Args:
        input_resolution (tuple[int]): Resolution of input feature.
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(4 * dim)

    def forward(self, x):
        """
        x: B, H*W, C
        """
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"
        assert H % 2 == 0 and W % 2 == 0, f"x size ({H}*{W}) are not even."

        x = x.view(B, H, W, C)

        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.norm(x)
        x = self.reduction(x)

        return x

```

​	4. 然后每个stage包含相同配置的transformer blocks，stage1到stage4的特征图分辨率分别是原图的![[公式]](https://www.zhihu.com/equation?tex=1%2F4)，![[公式]](https://www.zhihu.com/equation?tex=1%2F8)，![[公式]](https://www.zhihu.com/equation?tex=1%2F16)，![[公式]](https://www.zhihu.com/equation?tex=1%2F32)，这样Swin Transformer就非常容易应用在基于FPN的dense prediction任务中； （5)最后对所有的patch  embeddings求平均，即CNN中常用的global average pooling，然后送入一个linear  classifier进行分类。这和ViT采用class token来做分类不一样。

 

















# Abstract

提出了一种新的vision transformer，可以作为计算机视觉的通用骨干。Transformer从语言到视觉的适应挑战来自于两个领域的差异，例如视觉实体的规模变化很大，图像中的像素相对于文本中的单词的分辨率较高。

为了解决这些差异，我们提出了一个分层的Transformer，它的表示是用移位的窗口计算的。移位窗口方案将自注意计算限制在非重叠的局部窗口上，同时允许跨窗口连接，从而提高了效率。这种层次结构具有在不同尺度下建模的灵活性，并具有相对于图像大小的线性计算复杂度。Swin Transformer的这些特性使其兼容广泛的视觉任务，包括图像分类(ImageNet-1K上的86.4 top-1准确率)和密集的预测任务，如目标检测(COCO测试开发上的58.7框AP和51.1掩模AP)和语义分割(ADE20K val上的53.5 mIoU)。它的性能远远超过了之前的最先进的+2.7盒AP和+2.6掩模AP在COCO上，+3.2 mIoU在ADE20K上，显示了基于transformer的模型作为视觉骨干的潜力。

The code and models will be made publicly available at https://github.com/microsoft/Swin-Transformer.







# Introduction



在本文中，我们寻求扩展Transformer的适用性，使其可以作为计算机视觉的通用主干，就像它对NLP和cnn在视觉中的作用一样.

我们观察到，将其在语言领域的高表现转移到视觉领域的重大挑战可以用两种模式之间的差异来解释。  

1. 在现有的基于transformer的模型中[61,19]，令牌都是固定比例的，这一特性不适合这些视觉应用。
2. 另一个区别是，与文本段落中的文字相比，图像中的像素分辨率要高得多。

语义分割等许多视觉任务都需要在像素级进行密集的预测，这对于Transformer在高分辨率图像上是非常棘手的，因为它的自注意计算复杂度是图像大小的二次倍

为了克服这些问题，我们提出了一个通用的Transformer主干，称为Swin Transformer，它构建了层次特征映射，对图像大小具有线性计算复杂度。



![image-20210804165927697](E:\typora 截图\image-20210804165927697.png)

如图1(a)所示，Swin Transformer从较小的补丁(用灰色标出)开始，逐步合并更深的Transformer层中相邻的补丁，从而构建了一个层次表示。

有了这些分层特征映射，Swin Transformer模型可以方便地利用高级技术进行密集预测，如特征金字塔网络(FPN)[41]或U-Net[50]。

线性计算复杂度是通过在非重叠窗口内计算局部自注意来实现的，该窗口对图像进行分割(用红色标出)。

每个窗口中的patch数量是固定的，因此复杂度与图像大小成线性关系。这些优点使Swin Transformer适合作为各种视觉任务的通用骨干，而之前基于Transformer的架构[19]只生成单一分辨率的特征图，具有二次复杂度。

Swin Transformer的一个关键设计元素是它在连续的自注意层之间的窗口分区的移位，如图2所示。

![image-20210804172037548](E:\typora 截图\image-20210804172037548.png)

移位的窗口连接上一层的窗口，在它们之间提供连接，显著增强建模能力(见表4)

这个策略对于现实世界的延迟也是有效的:一个窗口中的所有查询补丁共享相同的键set1，这有助于硬件中的内存访问。

相比之下，早期基于滑动窗口的自注意方法[32,49]在一般硬件上存在较低的延迟，因为不同的查询pixels2有不同的键集。

我们的实验表明，所提出的移位窗口方法比滑动窗口方法有更低的延迟，但在建模能力上相似

我们相信，一个跨计算机视觉和自然语言处理的统一架构可以使这两个领域受益，因为它将促进视觉和文本信号的联合建模，来自两个领域的建模知识可以更深入地共享。我们希望Swin Transformer在各种视觉问题上的强大表现可以在社区中推动这种信念，并鼓励视觉和语言信号的统一建模。



# 2. Related Work



**CNN and variants**

CNN 作为整个计算机视觉的标准网络模型.

虽然CNN已经存在了几十年[39]，但直到AlexNet[38]的引入，CNN才起飞并成为主流。

此后，人们提出了更深入、更有效的卷积神经架构来进一步推动计算机视觉中的深度学习波，如VGG[51]、GoogleNet[56]、ResNet[29]、DenseNet[33]、HRNet[62]、EfficientNet[57]。

除了这些架构上的进步，人们还在改进单个卷积层上做了很多工作，如深度卷积[67]和可变形卷积

虽然CNN及其变体仍然是计算机视觉应用的主要骨干架构，但我们强调了类似transformer的架构在视觉和语言之间统一建模方面的强大潜力。我们的工作在几个基本的视觉识别任务上取得了良好的表现，我们希望它将有助于建模的转变。

**self-attention based backbone architectures**

同样受到自注意层和Transformer架构在NLP领域的成功启发，一些作品使用自注意层来替代流行的ResNet中的部分或全部空间卷积层[32,49,77]。在这些工作中，在每个像素的局部窗口内计算自我注意，以加速优化[32]，并且它们实现了比相应的ResNet架构稍好一些的精度/FLOPs权衡。然而，它们昂贵的内存访问导致它们的实际延迟明显大于卷积网络[32]。我们建议在连续的层之间移动窗口，而不是使用滑动窗口，这允许在一般硬件中更有效的实现。

**Self-attention/Transformers to complement CNNs**

另一项工作是用self-attention layers 或 transformers 来扩充标准的CNN架构。自注意层通过提供编码远距离依赖或异构交互的能力，可以补充骨干网络[64,6,68,22,71,54]或头网络[31,26]。最近，Transformer中的编解码器设计已被应用于目标检测和实例分割任务[7,12,82,55]。我们的工作是探索对变形金刚进行基本视觉特征提取的改编，是对这些工作的补充。

**Transformer based vision backbones**

与我们工作最相关的是Vision Transformer (ViT)[19]及其后续产品[60、69、14、27、63]。ViT的开创性工作是直接在不重叠的中型图像斑块上应用Transformer架构进行图像分类。与卷积网络相比，它在图像分类上取得了令人印象深刻的速度-精度折衷。虽然ViT需要大规模的训练数据集(即JFT-300M)才能表现良好，但DeiT[60]引入了几种训练策略，允许使用较小的ImageNet-1K数据集也能有效地进行ViT。维特在图像分类的结果是鼓舞人心的,但它的架构不适合作为一个通用的骨干网络密集的视觉任务或当输入图像分辨率高,由于其低分辨率特征图谱与图像大小和二次增加复杂性。

将ViT模型应用于直接上采样或反卷积的目标检测和语义分割等密集视觉任务的研究较少，但性能相对较差[2,78]。与我们的工作并行的是一些修改ViT架构的方法[69,14,27]，以更好地进行图像分类。根据经验，我们发现我们的Swin Transformer架构在这些图像分类方法中实现了最佳的速度精度权衡，尽管我们的工作重点是通用性能，而不是专门针对分类。另一项同时进行的工作[63]探索了在变形金刚上构建多分辨率特征图的类似思路。它的复杂性仍然是图像大小的二次函数，而我们的是线性的，并且在局部操作，这在建模视觉信号的高相关性方面被证明是有益的[35,24,40]。我们的方法既高效又有效，在COCO目标检测和ADE20K语义分割上都达到了最先进的精度。



# 3. Method

### 3.1. Overall Architecture

图3展示了Swin Transformer体系结构的概述，其中演示了小型版本(SwinT)。它首先通过patch分割模块将输入的RGB图像分割成不重叠的patch，如ViT。每个patch被视为一个 token ，其特征被设置为原始像素RGB值的串联。在我们的实现中，我们使用补丁大小为4 4，因此每个补丁的特征维数为4 4 3 = 48。在这个原始值特征上应用线性嵌入层，将其投影到任意维度(表示为C)。

![image-20210805113445492](E:\typora 截图\image-20210805113445492.png)



在这些patch tokens 上应用了几个带有修改过的自注意计算的Transformer块(Swin Transformer块)。Transformer块保持令牌的数量 $(\frac{H}{4} \times \frac{W}{4}$​ ，并与线性嵌入一起称为 Stage 1。

为了生成层次的表示，随着网络的深入，paych merging layers 可以减少 tokens 的数量

第一patch合并层将每组2 × 2相邻patch的特征进行拼接，并对拼接后的4c维特征应用线性层。这将令牌的数量减少为2×2 = 4的倍数(2×分辨率的下采样)，并且输出维度被设置为2C。之后采用Swin Transformer block进行特征变换，分辨率保持在 $\frac{H}{8} \times \frac{H}{8}$. patch合并和特征变换的第一块被称为“Stage 2”。 该操作重复两次，分为 Stage 3 和 Stage 4，输出分辨率 分别为 $\frac {H}{16} \times \frac{H}{16}$ 和 $\frac{H}{32} \times \frac{H}{32}$. 这些阶段共同产生一个层次表示，具有与典型卷积网络 (如 VGG[51] 和 ResNet[29]) 相同的特征图分辨率。因此，该体系结构可以方便地替代现有的各种视觉任务方法中的骨干网。

**Swin Tansformer block**

Swin Transformer是通过将Transformer块中的标准多头自注意(MSA)模块替换为基于移位窗口的模块(见3.2节)而构建的，其他层保持不变。如图3(b)所示，Swin Transformer块由一个基于移位窗口的MSA模块组成，随后是一个介于GELU非线性的2层MLP。每个MSA模块和每个MLP前加LayerNorm (LN)层，每个模块后加残余连接。

###  3.2. Shifted Window based Self-Attention

标准的Transformer架构[61]及其对图像分类[19]的适应性都进行全局自注意，其中计算一个令牌与所有其他令牌之间的关系。全局计算导致了令牌数量的二次复杂性，使得它不适用于许多需要大量令牌集进行密集预测或表示高分辨率图像的视觉问题。

**Self-attention in non-overlapped windows**

为了高效建模，我们建议在局部窗口内计算自注意。窗口以非重叠的方式均匀地划分图像。假设每个窗口包含M M个patch，全局MSA模块和基于h w patch图像的一个窗口的计算复杂度为3

![image-20210805120346213](E:\typora 截图\image-20210805120346213.png)

其中，当M固定时(默认为7)，前者与patch number hw是二次的，后者是线性的。全局自注意计算对于一个大的硬件通常是负担不起的，而基于窗口的自注意是可伸缩的。

**Shifted window partitioning in successive blocks**

基于窗口的自我注意模块缺乏跨窗口的连接，这限制了它的建模能力。为了引入跨窗口连接，同时保持非重叠窗口的高效计算，我们提出了一种移位窗口分区方法，在连续的Swin Transformer块中交替使用两种分区配置。

如图2所示,第一个模块使用一个普通窗口划分策略从左上的像素, $8 \times 8$ 的feature map 被平均的划分为 $2 \times 2$ 个 $4 \times 4$ 的窗口。然后，下一个模块采用从上一层移出的窗口配置，将规则划分的窗口替换为 $(\lfloor {\frac{M}{2} \rfloor , \lfloor \frac{M}{2} \rfloor })$ 像素。

采用移位窗口分区方法，计算连续的Swin Transformer块为

![image-20210805121809650](E:\typora 截图\image-20210805121809650.png)

$\hat{z}^l$ 代表  (S)W-MSA 模块在快 l 的输出的features

$z^l$ 代表 MLP模块在 块 l 中的输出features

W-MSA和SW-MSA分别表示使用规则和移位窗口分区配置的基于窗口的多头自注意

移位窗口划分方法在上一层引入了相邻的非重叠窗口之间的连接，在图像分类、目标分割和语义分割中被发现是有效的，如表4所示

**Efficient batch computation for shifted configuration**























































































